{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":105267,"databundleVersionId":12693789,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üì¶ **Installing Unsloth (for Speedy Finetuning!)**\nWe kick things off by installing unsloth, a library renowned for accelerating the finetuning process of large language models. This step is crucial for efficient experimentation and faster results.\n","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install unsloth\n# !pip install transformers>=4.54.1","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-02T13:35:00.822822Z","iopub.execute_input":"2025-08-02T13:35:00.823227Z","iopub.status.idle":"2025-08-02T13:35:51.361501Z","shell.execute_reply.started":"2025-08-02T13:35:00.823211Z","shell.execute_reply":"2025-08-02T13:35:51.360518Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üì• **Updating Transformers and Timm**\nUpgrade `transformers` and `timm` libraries to ensure compatibility with the latest model features and performance improvements.\n","metadata":{}},{"cell_type":"code","source":"%%capture\n# !pip install --no-deps git+https://github.com/huggingface/transformers.git \n!pip install transformers -U\n!pip install --no-deps --upgrade timm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T13:36:15.507879Z","iopub.execute_input":"2025-08-02T13:36:15.508420Z","iopub.status.idle":"2025-08-02T13:36:22.086144Z","shell.execute_reply.started":"2025-08-02T13:36:15.508387Z","shell.execute_reply":"2025-08-02T13:36:22.084818Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üîç **Check Transformers Version**\nVerify that the installed `transformers` version is at least 0.34.1. If not, reinstall from the GitHub repo to get the latest fixes.","metadata":{}},{"cell_type":"code","source":"import pkg_resources\ntransformers_version = pkg_resources.get_distribution(\"transformers\").version\nprint(f\"Current Transformers version: {transformers_version}\")\n\ndef version_to_tuple(version_str):\n    return tuple(map(int, (version_str.split('.'))))\n\nif version_to_tuple(transformers_version) < version_to_tuple(\"0.34.1\"):\n    print(\"Transformers version is less than 0.34.1. Reinstalling...\")\n    !pip install --no-deps git+https://github.com/huggingface/transformers.git\nelse:\n    print(\"Transformers version is 0.34.1 or higher. No reinstallation needed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T13:36:29.310590Z","iopub.execute_input":"2025-08-02T13:36:29.311379Z","iopub.status.idle":"2025-08-02T13:36:29.317654Z","shell.execute_reply.started":"2025-08-02T13:36:29.311344Z","shell.execute_reply":"2025-08-02T13:36:29.317152Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ‚öôÔ∏è **Import Needed Packages and Setup Torch Dynamo**\nImport required Python packages and optimize Torch Dynamo configuration for faster inference.","metadata":{}},{"cell_type":"code","source":"from unsloth import FastModel\nimport torch\nfrom IPython.display import Image, display\nimport gc\nimport json\n\ntorch._dynamo.config.cache_size_limit = 256","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T13:43:34.906595Z","iopub.execute_input":"2025-08-02T13:43:34.907154Z","iopub.status.idle":"2025-08-02T13:43:34.910783Z","shell.execute_reply.started":"2025-08-02T13:43:34.907129Z","shell.execute_reply":"2025-08-02T13:43:34.910074Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üß† **Load GEMMA-3N Model with 4-bit Quantization and LoRA Finetuning**\nLoad the pretrained GEMMA-3N model and prepare it for efficient parameter tuning.\n","metadata":{}},{"cell_type":"code","source":"model, tokenizer = FastModel.from_pretrained(\n    model_name = \"unsloth/gemma-3n-E4B-it\",\n    dtype = None,\n    max_seq_length = 4096,\n    load_in_4bit = True,\n    full_finetuning = False,\n)\n\nmodel = FastModel.get_peft_model(\n    model,\n    finetune_vision_layers     = True, \n    finetune_language_layers   = True,  \n    finetune_attention_modules = True,  \n    finetune_mlp_modules       = True,  \n\n    r = 8,\n    lora_alpha = 8,\n    lora_dropout = 0,\n    bias = \"none\",\n    random_state = 3407,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T13:37:23.455951Z","iopub.execute_input":"2025-08-02T13:37:23.456262Z","iopub.status.idle":"2025-08-02T13:38:45.754819Z","shell.execute_reply.started":"2025-08-02T13:37:23.456241Z","shell.execute_reply":"2025-08-02T13:38:45.754005Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üîÑ **Inference Helper Function: `do_gemma_3n_inference`**\nDefines a reusable function for robust multimodal inference.\n","metadata":{}},{"cell_type":"code","source":"def do_gemma_3n_inference(model, tokenizer, messages, max_new_tokens = 4096):\n    inputs = tokenizer.apply_chat_template(\n        messages,\n        add_generation_prompt = True,\n        tokenize = True,\n        return_dict = True,\n        return_tensors = \"pt\",\n    ).to(\"cuda:0\")\n\n    generated_token_ids = model.generate(\n        **inputs,\n        max_new_tokens = max_new_tokens,\n        temperature = 1.0, top_p = 0.95, top_k = 64,\n    )\n\n    \n    generated_text = tokenizer.decode(generated_token_ids[0][len(inputs['input_ids'][0]):], skip_special_tokens=True)\n\n    del inputs\n    del generated_token_ids \n    torch.cuda.empty_cache()\n    gc.collect()\n\n    return generated_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T13:38:53.034936Z","iopub.execute_input":"2025-08-02T13:38:53.035220Z","iopub.status.idle":"2025-08-02T13:38:53.040837Z","shell.execute_reply.started":"2025-08-02T13:38:53.035199Z","shell.execute_reply":"2025-08-02T13:38:53.040111Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üñºÔ∏è **Display Medical Report Image**\nDisplay the report image directly for reference.\n","metadata":{}},{"cell_type":"code","source":"report_image = \"https://raw.githubusercontent.com/DeepLumiere/MedOCR/master/beta/img.png\"\n\n# Display the image directly from the URL\ndisplay(Image(url=report_image))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T13:39:00.731317Z","iopub.execute_input":"2025-08-02T13:39:00.731758Z","iopub.status.idle":"2025-08-02T13:39:00.738359Z","shell.execute_reply.started":"2025-08-02T13:39:00.731715Z","shell.execute_reply":"2025-08-02T13:39:00.737652Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üìù **Extract Distinct Medical Test Names**\nAsk the model to extract all unique medical test names from the uploaded report image.\n","metadata":{}},{"cell_type":"code","source":"report_image = \"https://raw.githubusercontent.com/DeepLumiere/MedOCR/master/beta/img.png\"\n\nlist_tests_messages = [{\n    \"role\": \"user\",\n    \"content\": [\n        { \"type\": \"image\", \"image\": report_image },\n        { \"type\": \"text\", \"text\": \"From this medical report image, please list all the distinct medical test names you can identify. Present them as a comma-separated list.\" }\n    ]\n}]\n\nprint(\"--- Extracting Test Features from the Report Image ---\")\nprint(\"Please wait, the model is analyzing the image...\")\n\nraw_test_list_output = do_gemma_3n_inference(model, tokenizer, list_tests_messages, max_new_tokens=128)\n\ntest_names = []\nif ',' in raw_test_list_output:\n    test_names = [name.strip() for name in raw_test_list_output.split(',') if name.strip()]\nelse:\n    for line in raw_test_list_output.split('\\n'):\n        clean_line = line.strip()\n        if clean_line and (clean_line.startswith(('* ', '- ', '‚Ä¢ ')) or (clean_line[0].isdigit() and '. ' in clean_line[:3])):\n            clean_line = clean_line.split('. ', 1)[-1].strip() \n        if clean_line:\n            test_names.append(clean_line)\n\nprint(\"\\n--- Identified Test Features (Copy/Type one into the next cell) ---\")\nitems_per_row = 3\nfor i, test_name in enumerate(test_names):\n    print(f\"{test_name:<30}\", end=\"\") \n    if (i + 1) % items_per_row == 0:\n        print() \nprint()\nprint(\"---------------------------------------------------------------\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T13:39:15.747107Z","iopub.execute_input":"2025-08-02T13:39:15.747376Z","iopub.status.idle":"2025-08-02T13:41:06.634610Z","shell.execute_reply.started":"2025-08-02T13:39:15.747356Z","shell.execute_reply":"2025-08-02T13:41:06.633913Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üîé **Detailed Review for a Selected Test (e.g., MCHC)**\nQuery the model with a specific test name to find value, normal range review, and possible abnormality reasons.\n\n","metadata":{}},{"cell_type":"code","source":"finder = \"MCHC\"# input(\"Enter the test name you want to review: \")\n\nmessages = [{\n    \"role\" : \"user\",\n    \"content\": [\n        { \"type\": \"image\", \"image\" : report_image },\n        { \"type\": \"text\",  \"text\" : finder + \" . Find value, review whether it's in normal range, if not give potential reasons.\" }\n    ]\n}]\n\ninputs = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt = True,\n    return_tensors = \"pt\",\n    tokenize = True,\n    return_dict = True,\n).to(\"cuda:0\")\noutputs = model.generate(\n    **inputs,\n    max_new_tokens = 2048,\n    temperature = 1.0, top_p = 0.95, top_k = 64,\n)\ntokenizer.batch_decode(outputs)\n\nlabreport = do_gemma_3n_inference(model, tokenizer, messages, max_new_tokens = 128)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T13:41:24.959603Z","iopub.execute_input":"2025-08-02T13:41:24.960384Z","iopub.status.idle":"2025-08-02T13:42:49.034474Z","shell.execute_reply.started":"2025-08-02T13:41:24.960357Z","shell.execute_reply":"2025-08-02T13:42:49.033817Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üìã **Display the Detailed Test Review Output**\nPrint and review the model‚Äôs answer for clinical context.\n","metadata":{}},{"cell_type":"code","source":"print(labreport)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T13:43:00.877794Z","iopub.execute_input":"2025-08-02T13:43:00.878064Z","iopub.status.idle":"2025-08-02T13:43:00.882308Z","shell.execute_reply.started":"2025-08-02T13:43:00.878047Z","shell.execute_reply":"2025-08-02T13:43:00.881461Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üíæ **Save the Fine-Tuned Model and Tokenizer Locally**\nStore the model and tokenizer assets for future inference and deployment.\n","metadata":{}},{"cell_type":"code","source":"model.save_pretrained(\"gemma-3n\") \ntokenizer.save_pretrained(\"gemma-3n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T13:43:13.859875Z","iopub.execute_input":"2025-08-02T13:43:13.860495Z","iopub.status.idle":"2025-08-02T13:43:18.249082Z","shell.execute_reply.started":"2025-08-02T13:43:13.860474Z","shell.execute_reply":"2025-08-02T13:43:18.248458Z"}},"outputs":[],"execution_count":null}]}